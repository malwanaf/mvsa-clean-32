{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Implementasi Low-Dimensional Computing (LDC) End-to-End dengan UCI-HAR\n",
        "\n",
        "Skrip ini membangun dan melatih model LDC menggunakan dataset UCI Human\n",
        "Activity Recognition, sesuai dengan paper \"A Brain-Inspired Low-Dimensional\n",
        "Computing Classifier for Inference on Tiny Devices\".\n",
        "\n",
        "Perubahan utama dari versi Fashion-MNIST adalah pada pipeline data:\n",
        "1.  Mengunduh dan mengekstrak dataset UCI-HAR.\n",
        "2.  Memuat data dari file teks.\n",
        "3.  Menskalakan fitur ke rentang [0, 255] dan mengkuantisasinya,\n",
        "    sesuai metodologi paper.\n",
        "4.  Menyesuaikan hyperparameter model untuk dataset ini.\n",
        "\n",
        "Setelah training, skrip ini akan mengekspor vektor-vektor yang telah\n",
        "dipelajari (V, F, dan C) dan memverifikasi akurasinya menggunakan inferensi manual.\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Setup dan Import Library\n",
        "# =============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Definisi Komponen Model LDC (Dengan Perbaikan)\n",
        "# =============================================================================\n",
        "\n",
        "# Fungsi aktivasi kustom untuk memastikan konsistensi (sign(0) = 1)\n",
        "class SignWithZeroToOne(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        return torch.where(x == 0, 1.0, torch.sign(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Ini adalah bagian dari Straight-Through Estimator (STE)\n",
        "        # Melewatkan gradien seolah-olah fungsi ini adalah identitas\n",
        "        x, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        # Gradien hanya mengalir pada nilai antara -1 dan 1\n",
        "        grad_input[x.abs() > 1.0] = 0\n",
        "        return grad_input\n",
        "\n",
        "custom_sign = SignWithZeroToOne.apply\n",
        "\n",
        "class BinaryLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Lapisan linear biner menggunakan Straight-Through Estimator (STE)\n",
        "    untuk memungkinkan training dengan backpropagation.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(BinaryLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        real_weights = self.weight\n",
        "        # Binarisasi menggunakan STE yang lebih eksplisit\n",
        "        binary_weights = custom_sign(real_weights)\n",
        "        y = F.linear(x, binary_weights)\n",
        "        return y\n",
        "\n",
        "class ValueBox(nn.Module):\n",
        "    \"\"\"\n",
        "    Jaringan Saraf kecil untuk memetakan nilai skalar (fitur) ke vektor.\n",
        "    Arsitektur: 1 -> 20 -> Dv\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim):\n",
        "        super(ValueBox, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1, 20),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(20, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class LDC_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Model LDC End-to-End yang menggabungkan semua komponen.\n",
        "    **Diperbarui dengan Batch Normalization untuk stabilitas.**\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, dv, df, num_classes, dropout_rate=0.5):\n",
        "        super(LDC_Model, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.dv = dv\n",
        "        self.df = df\n",
        "\n",
        "        self.value_box = ValueBox(output_dim=dv)\n",
        "\n",
        "        feature_layer_input_dim = num_features * dv\n",
        "        # PERBAIKAN: Menambahkan BatchNorm setelah ValueBox\n",
        "        self.bn_value = nn.BatchNorm1d(feature_layer_input_dim)\n",
        "\n",
        "        self.feature_layer = BinaryLinear(feature_layer_input_dim, df)\n",
        "\n",
        "        # PERBAIKAN: Menambahkan BatchNorm setelah FeatureLayer\n",
        "        self.bn_feature = nn.BatchNorm1d(df)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.class_layer = BinaryLinear(df, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        # Input 'x' sudah dikuantisasi ke 0-255.\n",
        "        # Normalisasi ke [-1, 1] untuk input ValueBox\n",
        "        x_normalized = (x / 127.5) - 1.0\n",
        "        x_reshaped = x_normalized.unsqueeze(-1)\n",
        "        value_vectors = self.value_box(x_reshaped)\n",
        "\n",
        "        combined_value_vectors = value_vectors.view(batch_size, -1)\n",
        "        combined_value_vectors = self.bn_value(combined_value_vectors)\n",
        "\n",
        "        sample_vector = self.feature_layer(combined_value_vectors)\n",
        "        sample_vector = self.bn_feature(sample_vector)\n",
        "\n",
        "        sample_vector = self.dropout(sample_vector)\n",
        "        sample_vector_bin = custom_sign(sample_vector) # Gunakan fungsi sign kustom\n",
        "\n",
        "        output_logits = self.class_layer(sample_vector_bin)\n",
        "        return output_logits\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Helper Function untuk Data\n",
        "# =============================================================================\n",
        "\n",
        "def download_and_extract_zip(url, dest_path):\n",
        "    \"\"\"Mengunduh dan mengekstrak file ZIP dataset.\"\"\"\n",
        "    zip_filename = os.path.join(dest_path, 'UCI_HAR_Dataset.zip')\n",
        "    extract_folder = os.path.join(dest_path, 'UCI HAR Dataset')\n",
        "\n",
        "    if os.path.exists(extract_folder):\n",
        "        print(f\"Folder dataset sudah ada di '{extract_folder}'\")\n",
        "        return extract_folder\n",
        "\n",
        "    if not os.path.exists(dest_path):\n",
        "        os.makedirs(dest_path)\n",
        "\n",
        "    print(f\"Mengunduh dataset dari {url}...\")\n",
        "    try:\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(zip_filename, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Gagal mengunduh. Silakan unduh manual dari {url} dan ekstrak ke '{extract_folder}'. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    print(\"Ekstraksi file ZIP...\")\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dest_path)\n",
        "\n",
        "    os.remove(zip_filename)\n",
        "    print(\"Dataset siap.\")\n",
        "    return extract_folder\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Class untuk Inferensi Manual (untuk Verifikasi)\n",
        "# =============================================================================\n",
        "\n",
        "class LDCInference:\n",
        "    \"\"\"Class untuk melakukan inferensi LDC manual menggunakan NumPy.\"\"\"\n",
        "    def __init__(self, vector_dir, training_data_for_scaler):\n",
        "        # PERBAIKAN: Memuat vektor V REAL-VALUED\n",
        "        self.V_real = np.load(os.path.join(vector_dir, 'V_real_value_vectors.npy'))\n",
        "        self.F = np.load(os.path.join(vector_dir, 'F_feature_vectors.npy'))\n",
        "        self.C = np.load(os.path.join(vector_dir, 'C_class_vectors.npy'))\n",
        "\n",
        "        # Memuat parameter BatchNorm dan memindahkannya ke CPU\n",
        "        bn_value_state = torch.load(os.path.join(vector_dir, 'bn_value_state_dict.pth'), map_location='cpu')\n",
        "        self.bn_value_mean = bn_value_state['running_mean'].numpy()\n",
        "        self.bn_value_var = bn_value_state['running_var'].numpy()\n",
        "        self.bn_value_weight = bn_value_state['weight'].numpy()\n",
        "        self.bn_value_bias = bn_value_state['bias'].numpy()\n",
        "\n",
        "        bn_feature_state = torch.load(os.path.join(vector_dir, 'bn_feature_state_dict.pth'), map_location='cpu')\n",
        "        self.bn_feature_mean = bn_feature_state['running_mean'].numpy()\n",
        "        self.bn_feature_var = bn_feature_state['running_var'].numpy()\n",
        "        self.bn_feature_weight = bn_feature_state['weight'].numpy()\n",
        "        self.bn_feature_bias = bn_feature_state['bias'].numpy()\n",
        "\n",
        "        self.epsilon = 1e-5 # Epsilon default BatchNorm\n",
        "\n",
        "        # Menggunakan metode normalisasi manual yang sama seperti di training\n",
        "        self.min_val = np.min(training_data_for_scaler, axis=0)\n",
        "        self.max_val = np.max(training_data_for_scaler, axis=0)\n",
        "        self.range_val = self.max_val - self.min_val\n",
        "        self.range_val[self.range_val == 0] = 1 # Hindari pembagian dengan nol\n",
        "        self.num_value = 256\n",
        "\n",
        "    def predict_single(self, raw_sample_vector):\n",
        "        # Normalisasi dan Kuantisasi\n",
        "        normalized_sample = (raw_sample_vector - self.min_val) / self.range_val\n",
        "        quantized_sample = np.clip((normalized_sample * (self.num_value - 1)), 0, self.num_value - 1).astype(int)\n",
        "\n",
        "        # Langkah 1: ValueBox (menggunakan Vektor REAL)\n",
        "        value_vectors_combined = self.V_real[quantized_sample].flatten()\n",
        "\n",
        "        # Langkah 2: Terapkan BatchNorm untuk Value\n",
        "        bn_value_out = (value_vectors_combined - self.bn_value_mean) / np.sqrt(self.bn_value_var + self.epsilon)\n",
        "        bn_value_out = bn_value_out * self.bn_value_weight + self.bn_value_bias\n",
        "\n",
        "        # Langkah 3: FeatureLayer\n",
        "        sample_vector_S_real = bn_value_out @ self.F.T\n",
        "\n",
        "        # Langkah 4: Terapkan BatchNorm untuk Feature\n",
        "        bn_feature_out = (sample_vector_S_real - self.bn_feature_mean) / np.sqrt(self.bn_feature_var + self.epsilon)\n",
        "        bn_feature_out = bn_feature_out * self.bn_feature_weight + self.bn_feature_bias\n",
        "\n",
        "        # Langkah 5: Binarisasi\n",
        "        sample_vector_S_bin = np.sign(bn_feature_out)\n",
        "        sample_vector_S_bin[sample_vector_S_bin == 0] = 1\n",
        "\n",
        "        # Langkah 6: ClassLayer\n",
        "        similarity_scores = sample_vector_S_bin @ self.C.T\n",
        "        return np.argmax(similarity_scores)\n",
        "\n",
        "# =============================================================================\n",
        "# 5. Fungsi Training dan Eksekusi Utama\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fungsi utama untuk menjalankan seluruh pipeline:\n",
        "    data loading, inisialisasi model, training, ekspor, dan verifikasi.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Menggunakan device: {device}')\n",
        "\n",
        "    # --- Persiapan Data (UCI-HAR) ---\n",
        "    zip_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip'\n",
        "    data_dir = download_and_extract_zip(zip_url, './data')\n",
        "    if data_dir is None: return\n",
        "\n",
        "    X_train_raw = np.loadtxt(os.path.join(data_dir, 'train', 'X_train.txt'), dtype=np.float32)\n",
        "    y_train_raw = np.loadtxt(os.path.join(data_dir, 'train', 'y_train.txt'), dtype=np.int64)\n",
        "    X_test_raw = np.loadtxt(os.path.join(data_dir, 'test', 'X_test.txt'), dtype=np.float32)\n",
        "    y_test_raw = np.loadtxt(os.path.join(data_dir, 'test', 'y_test.txt'), dtype=np.int64)\n",
        "\n",
        "    # Normalisasi dan Kuantisasi Manual\n",
        "    num_value = 256\n",
        "    min_val = np.min(X_train_raw, axis=0)\n",
        "    max_val = np.max(X_train_raw, axis=0)\n",
        "    range_val = max_val - min_val\n",
        "    range_val[range_val == 0] = 1\n",
        "\n",
        "    X_train_normalized = (X_train_raw - min_val) / range_val\n",
        "    X_test_normalized = (X_test_raw - min_val) / range_val\n",
        "\n",
        "    X_train_quantized = np.clip((X_train_normalized * (num_value - 1)), 0, num_value - 1).astype(int)\n",
        "    X_test_quantized = np.clip((X_test_normalized * (num_value - 1)), 0, num_value - 1).astype(int)\n",
        "\n",
        "    X_train = X_train_quantized.astype(np.float32)\n",
        "    X_test = X_test_quantized.astype(np.float32)\n",
        "\n",
        "    y_train = y_train_raw - 1\n",
        "    y_test = y_test_raw - 1\n",
        "\n",
        "    # --- Hyperparameter Sesuai Paper LDC ---\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 0.0001\n",
        "    DROPOUT_RATE = 0.5\n",
        "\n",
        "    train_set = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    test_set = torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(f'Jumlah data training: {len(train_set)}')\n",
        "    print(f'Jumlah data testing: {len(test_set)}\\n')\n",
        "\n",
        "    # --- Training Model LDC ---\n",
        "    NUM_FEATURES = 561\n",
        "    NUM_CLASSES = 6\n",
        "    DV = 8\n",
        "    DF = 128 # Sesuai paper\n",
        "\n",
        "    model = LDC_Model(NUM_FEATURES, DV, DF, NUM_CLASSES, dropout_rate=DROPOUT_RATE).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    print(\"Model LDC disesuaikan dengan paper untuk UCI-HAR:\")\n",
        "    print(f\"Features: {NUM_FEATURES}, Classes: {NUM_CLASSES}, Dv: {DV}, Df: {DF}\")\n",
        "    print(\"\\nMemulai proses training...\")\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
        "\n",
        "        for data, labels in progress_bar:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f'{running_loss / (progress_bar.n + 1):.4f}'})\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(device), labels.to(device)\n",
        "                outputs = model(data)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        current_accuracy = 100 * correct / total\n",
        "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(train_loader):.4f}, Akurasi Test (PyTorch): {current_accuracy:.2f}%')\n",
        "\n",
        "        if current_accuracy > best_accuracy:\n",
        "            best_accuracy = current_accuracy\n",
        "            # Simpan model terbaik untuk diekspor nanti\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"\\nAkurasi terbaik yang dicapai: {best_accuracy:.2f}%\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 6. Ekspor Vektor Model (dari model terbaik)\n",
        "    # =============================================================================\n",
        "    print(\"\\nMemuat model terbaik untuk ekspor...\")\n",
        "    # Buat instance model baru dan muat state_dict terbaik\n",
        "    best_model = LDC_Model(NUM_FEATURES, DV, DF, NUM_CLASSES, dropout_rate=DROPOUT_RATE).to(device)\n",
        "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
        "    best_model.eval()\n",
        "\n",
        "    print(\"Mengekspor vektor model...\")\n",
        "\n",
        "    output_dir = \"exported_vectors_ucihar\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        possible_values = torch.arange(0, 256, device=device).float().unsqueeze(1)\n",
        "        normalized_values = (possible_values / 127.5) - 1.0\n",
        "        # PERBAIKAN: Ekspor Vektor V REAL-VALUED\n",
        "        value_vectors_V_real = best_model.value_box(normalized_values)\n",
        "        np.save(os.path.join(output_dir, 'V_real_value_vectors.npy'), value_vectors_V_real.cpu().numpy())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feature_vectors_F = custom_sign(best_model.feature_layer.weight)\n",
        "        np.save(os.path.join(output_dir, 'F_feature_vectors.npy'), feature_vectors_F.cpu().numpy())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        class_vectors_C = custom_sign(best_model.class_layer.weight)\n",
        "        np.save(os.path.join(output_dir, 'C_class_vectors.npy'), class_vectors_C.cpu().numpy())\n",
        "\n",
        "    # Ekspor parameter BatchNorm\n",
        "    torch.save(best_model.bn_value.state_dict(), os.path.join(output_dir, 'bn_value_state_dict.pth'))\n",
        "    torch.save(best_model.bn_feature.state_dict(), os.path.join(output_dir, 'bn_feature_state_dict.pth'))\n",
        "\n",
        "    print(f\"Vektor dan parameter BatchNorm berhasil diekspor ke folder '{output_dir}'.\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 7. Verifikasi Menggunakan Inferensi Manual\n",
        "    # =============================================================================\n",
        "    print(\"\\nMemulai verifikasi dengan inferensi manual pada seluruh data tes...\")\n",
        "    engine = LDCInference(output_dir, X_train_raw)\n",
        "\n",
        "    manual_correct = 0\n",
        "    for i in tqdm(range(len(X_test_raw)), desc=\"Verifikasi Manual\"):\n",
        "        prediction = engine.predict_single(X_test_raw[i])\n",
        "        if prediction == y_test[i]:\n",
        "            manual_correct += 1\n",
        "\n",
        "    manual_accuracy = 100 * manual_correct / len(X_test_raw)\n",
        "\n",
        "    print(\"\\n--- Hasil Verifikasi ---\")\n",
        "    print(f\"Akurasi model PyTorch (terbaik): {best_accuracy:.2f}%\")\n",
        "    print(f\"Akurasi inferensi manual (NumPy):      {manual_accuracy:.2f}%\")\n",
        "\n",
        "    # Toleransi bisa sedikit lebih besar karena perbedaan minor antara STE dan NumPy\n",
        "    if abs(best_accuracy - manual_accuracy) < 0.5:\n",
        "        print(\"✅ Verifikasi BERHASIL: Akurasi cocok.\")\n",
        "    else:\n",
        "        print(\"❌ Verifikasi GAGAL: Terdapat perbedaan signifikan pada akurasi.\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 8. (FIXED-POINT) Ekspor Parameter & Data Uji untuk Implementasi C\n",
        "    # =============================================================================\n",
        "\n",
        "\n",
        "    print(\"\\nMemulai ekspor untuk implementasi Fixed-Point...\")\n",
        "\n",
        "    # --- Konfigurasi ---\n",
        "    SCALE_FACTOR = 4096\n",
        "    # Ekspor sampel pertama (indeks 0) dari data tes\n",
        "    TEST_SAMPLE_IDX = 0\n",
        "\n",
        "    # --- Perhitungan Parameter Fixed-Point ---\n",
        "    # Variabel X_train_raw, X_test_raw, dan y_test sudah ada dari proses sebelumnya\n",
        "    min_val = np.min(X_train_raw, axis=0)\n",
        "    max_val = np.max(X_train_raw, axis=0)\n",
        "    range_val = max_val - min_val\n",
        "    range_val[range_val == 0] = 1e-9\n",
        "\n",
        "    min_val_scaled = (min_val * SCALE_FACTOR).astype(np.int32)\n",
        "    inv_range_val_scaled = ((1.0 / range_val) * SCALE_FACTOR).astype(np.int32)\n",
        "\n",
        "    # Ambil data mentah (float) dan label yang sesuai\n",
        "    test_data_sample_raw = X_test_raw[TEST_SAMPLE_IDX]\n",
        "    # REPLACE IT WITH THIS:\n",
        "    # Use the zero-indexed y_test, which is consistent with the model's training\n",
        "    y_test = y_test_raw - 1\n",
        "    test_data_actual_label = y_test[TEST_SAMPLE_IDX]\n",
        "\n",
        "    print(f\"Data uji mentah (float) yang diekspor adalah untuk kelas: {test_data_actual_label}\")\n",
        "\n",
        "    # --- Proses Ekspor ke File C ---\n",
        "\n",
        "    # Ekspor Parameter Model (model_params_scaled.h)\n",
        "    header_path = \"model_params_scaled.h\"\n",
        "    with open(header_path, \"w\") as f:\n",
        "        f.write(\"#ifndef MODEL_PARAMS_SCALED_H_\\n#define MODEL_PARAMS_SCALED_H_\\n\\n#include <stdint.h>\\n\\n\")\n",
        "        f.write(f\"#define FIXED_POINT_SCALE_FACTOR {SCALE_FACTOR}\\n\\n\")\n",
        "        f.write(f\"const int32_t min_val_scaled[{len(min_val_scaled)}] = {{ \")\n",
        "        f.write(\", \".join(map(str, min_val_scaled)))\n",
        "        f.write(\" };\\n\\n\")\n",
        "        f.write(f\"const int32_t inv_range_val_scaled[{len(inv_range_val_scaled)}] = {{ \")\n",
        "        f.write(\", \".join(map(str, inv_range_val_scaled)))\n",
        "        f.write(\" };\\n\\n#endif // MODEL_PARAMS_SCALED_H_\\n\")\n",
        "    print(f\"Berhasil mengekspor '{header_path}'\")\n",
        "\n",
        "    # Ekspor Data Uji (test_data.h dan test_data.c)\n",
        "    header_path = \"test_data.h\"\n",
        "    with open(header_path, \"w\") as f:\n",
        "        f.write(\"#ifndef TEST_DATA_H_\\n#define TEST_DATA_H_\\n\\n#include <stdint.h>\\n\\n\")\n",
        "        f.write(f\"#define TEST_DATA_SAMPLE_LENGTH {len(test_data_sample_raw)}\\n\")\n",
        "        f.write(\"extern const float test_data_sample[];\\n\")\n",
        "        f.write(f\"extern const uint8_t test_data_actual_label;\\n\\n#endif // TEST_DATA_H_\\n\")\n",
        "    print(f\"Berhasil mengekspor '{header_path}'\")\n",
        "\n",
        "    source_path = \"test_data.c\"\n",
        "    with open(source_path, \"w\") as f:\n",
        "        f.write('#include \"test_data.h\"\\n\\n')\n",
        "        f.write(f\"const uint8_t test_data_actual_label = {test_data_actual_label};\\n\\n\")\n",
        "        f.write(f\"const float test_data_sample[TEST_DATA_SAMPLE_LENGTH] = {{\\n    \")\n",
        "        f.write(\", \".join([f\"{val:.8f}f\" for val in test_data_sample_raw]))\n",
        "        f.write(\"\\n};\\n\")\n",
        "    print(f\"Berhasil mengekspor '{source_path}'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menggunakan device: cuda\n",
            "Folder dataset sudah ada di './data/UCI HAR Dataset'\n",
            "Jumlah data training: 7352\n",
            "Jumlah data testing: 2947\n",
            "\n",
            "Model LDC disesuaikan dengan paper untuk UCI-HAR:\n",
            "Features: 561, Classes: 6, Dv: 8, Df: 128\n",
            "\n",
            "Memulai proses training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100: 100%|██████████| 115/115 [00:00<00:00, 217.12it/s, loss=3.1214]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 3.0671, Akurasi Test (PyTorch): 82.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/100: 100%|██████████| 115/115 [00:00<00:00, 214.11it/s, loss=1.7490]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Loss: 1.7033, Akurasi Test (PyTorch): 89.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/100: 100%|██████████| 115/115 [00:00<00:00, 215.33it/s, loss=1.4755]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/100], Loss: 1.4242, Akurasi Test (PyTorch): 90.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/100: 100%|██████████| 115/115 [00:00<00:00, 212.41it/s, loss=1.3560]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/100], Loss: 1.3206, Akurasi Test (PyTorch): 88.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/100: 100%|██████████| 115/115 [00:00<00:00, 214.39it/s, loss=1.1435]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/100], Loss: 1.1237, Akurasi Test (PyTorch): 54.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/100: 100%|██████████| 115/115 [00:00<00:00, 215.62it/s, loss=1.1622]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/100], Loss: 1.1319, Akurasi Test (PyTorch): 90.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/100: 100%|██████████| 115/115 [00:00<00:00, 212.11it/s, loss=1.1643]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/100], Loss: 1.1542, Akurasi Test (PyTorch): 87.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/100: 100%|██████████| 115/115 [00:00<00:00, 215.83it/s, loss=1.0355]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/100], Loss: 1.0085, Akurasi Test (PyTorch): 91.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/100: 100%|██████████| 115/115 [00:00<00:00, 209.52it/s, loss=1.0049]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/100], Loss: 0.9612, Akurasi Test (PyTorch): 90.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/100: 100%|██████████| 115/115 [00:00<00:00, 216.46it/s, loss=0.8651]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.8425, Akurasi Test (PyTorch): 93.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100: 100%|██████████| 115/115 [00:00<00:00, 215.25it/s, loss=0.8788]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/100], Loss: 0.8482, Akurasi Test (PyTorch): 92.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100: 100%|██████████| 115/115 [00:00<00:00, 210.53it/s, loss=0.6881]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/100], Loss: 0.6642, Akurasi Test (PyTorch): 88.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/100: 100%|██████████| 115/115 [00:00<00:00, 214.93it/s, loss=0.7759]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/100], Loss: 0.7556, Akurasi Test (PyTorch): 93.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/100: 100%|██████████| 115/115 [00:00<00:00, 208.20it/s, loss=0.7134]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/100], Loss: 0.6886, Akurasi Test (PyTorch): 94.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/100: 100%|██████████| 115/115 [00:00<00:00, 212.91it/s, loss=0.7316]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/100], Loss: 0.7188, Akurasi Test (PyTorch): 85.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/100: 100%|██████████| 115/115 [00:00<00:00, 207.31it/s, loss=0.6951]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/100], Loss: 0.6468, Akurasi Test (PyTorch): 93.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/100: 100%|██████████| 115/115 [00:00<00:00, 179.24it/s, loss=0.8909]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/100], Loss: 0.7437, Akurasi Test (PyTorch): 93.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/100: 100%|██████████| 115/115 [00:00<00:00, 199.86it/s, loss=0.5531]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/100], Loss: 0.5098, Akurasi Test (PyTorch): 92.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/100: 100%|██████████| 115/115 [00:00<00:00, 180.85it/s, loss=0.6940]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/100], Loss: 0.6155, Akurasi Test (PyTorch): 93.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/100: 100%|██████████| 115/115 [00:00<00:00, 168.18it/s, loss=0.5771]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 0.5470, Akurasi Test (PyTorch): 94.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/100: 100%|██████████| 115/115 [00:00<00:00, 213.20it/s, loss=0.5063]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/100], Loss: 0.4930, Akurasi Test (PyTorch): 90.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/100: 100%|██████████| 115/115 [00:00<00:00, 210.39it/s, loss=0.5236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/100], Loss: 0.5100, Akurasi Test (PyTorch): 92.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/100: 100%|██████████| 115/115 [00:00<00:00, 214.95it/s, loss=0.4644]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/100], Loss: 0.4564, Akurasi Test (PyTorch): 91.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/100: 100%|██████████| 115/115 [00:00<00:00, 213.41it/s, loss=0.4223]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/100], Loss: 0.4040, Akurasi Test (PyTorch): 89.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/100: 100%|██████████| 115/115 [00:00<00:00, 206.11it/s, loss=0.4612]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/100], Loss: 0.4452, Akurasi Test (PyTorch): 84.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/100: 100%|██████████| 115/115 [00:00<00:00, 213.22it/s, loss=0.4726]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/100], Loss: 0.4602, Akurasi Test (PyTorch): 86.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/100: 100%|██████████| 115/115 [00:00<00:00, 213.22it/s, loss=0.4554]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/100], Loss: 0.4435, Akurasi Test (PyTorch): 94.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/100: 100%|██████████| 115/115 [00:00<00:00, 214.52it/s, loss=0.4404]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/100], Loss: 0.4289, Akurasi Test (PyTorch): 93.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/100: 100%|██████████| 115/115 [00:00<00:00, 211.57it/s, loss=0.4075]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/100], Loss: 0.3862, Akurasi Test (PyTorch): 91.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/100: 100%|██████████| 115/115 [00:00<00:00, 211.42it/s, loss=0.3264]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/100], Loss: 0.3178, Akurasi Test (PyTorch): 94.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/100: 100%|██████████| 115/115 [00:00<00:00, 207.44it/s, loss=0.2430]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/100], Loss: 0.2367, Akurasi Test (PyTorch): 95.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/100: 100%|██████████| 115/115 [00:00<00:00, 212.27it/s, loss=0.1635]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/100], Loss: 0.1607, Akurasi Test (PyTorch): 96.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/100: 100%|██████████| 115/115 [00:00<00:00, 209.44it/s, loss=0.1427]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/100], Loss: 0.1377, Akurasi Test (PyTorch): 95.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/100: 100%|██████████| 115/115 [00:00<00:00, 208.95it/s, loss=0.1378]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/100], Loss: 0.1295, Akurasi Test (PyTorch): 96.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/100: 100%|██████████| 115/115 [00:00<00:00, 207.97it/s, loss=0.1567]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/100], Loss: 0.1513, Akurasi Test (PyTorch): 95.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/100: 100%|██████████| 115/115 [00:00<00:00, 212.29it/s, loss=0.1496]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/100], Loss: 0.1444, Akurasi Test (PyTorch): 96.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/100: 100%|██████████| 115/115 [00:00<00:00, 182.49it/s, loss=0.1236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/100], Loss: 0.1053, Akurasi Test (PyTorch): 96.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/100: 100%|██████████| 115/115 [00:00<00:00, 190.62it/s, loss=0.1199]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/100], Loss: 0.1022, Akurasi Test (PyTorch): 95.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/100: 100%|██████████| 115/115 [00:00<00:00, 177.50it/s, loss=0.1495]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/100], Loss: 0.1287, Akurasi Test (PyTorch): 95.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/100: 100%|██████████| 115/115 [00:00<00:00, 168.22it/s, loss=0.1521]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/100], Loss: 0.1415, Akurasi Test (PyTorch): 95.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/100: 100%|██████████| 115/115 [00:00<00:00, 216.27it/s, loss=0.1183]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/100], Loss: 0.1162, Akurasi Test (PyTorch): 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/100: 100%|██████████| 115/115 [00:00<00:00, 208.79it/s, loss=0.1261]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/100], Loss: 0.1206, Akurasi Test (PyTorch): 96.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/100: 100%|██████████| 115/115 [00:00<00:00, 209.28it/s, loss=0.1216]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/100], Loss: 0.1174, Akurasi Test (PyTorch): 96.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/100: 100%|██████████| 115/115 [00:00<00:00, 213.34it/s, loss=0.1214]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/100], Loss: 0.1182, Akurasi Test (PyTorch): 95.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/100: 100%|██████████| 115/115 [00:00<00:00, 205.54it/s, loss=0.1287]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/100], Loss: 0.1208, Akurasi Test (PyTorch): 96.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/100: 100%|██████████| 115/115 [00:00<00:00, 213.11it/s, loss=0.1324]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/100], Loss: 0.1267, Akurasi Test (PyTorch): 95.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/100: 100%|██████████| 115/115 [00:00<00:00, 211.46it/s, loss=0.1394]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/100], Loss: 0.1345, Akurasi Test (PyTorch): 96.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/100: 100%|██████████| 115/115 [00:00<00:00, 206.94it/s, loss=0.1554]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/100], Loss: 0.1513, Akurasi Test (PyTorch): 96.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/100: 100%|██████████| 115/115 [00:00<00:00, 209.43it/s, loss=0.1122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/100], Loss: 0.1073, Akurasi Test (PyTorch): 95.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/100: 100%|██████████| 115/115 [00:00<00:00, 208.66it/s, loss=0.1042]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/100], Loss: 0.1006, Akurasi Test (PyTorch): 95.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 51/100: 100%|██████████| 115/115 [00:00<00:00, 210.11it/s, loss=0.1358]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [51/100], Loss: 0.1299, Akurasi Test (PyTorch): 96.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 52/100: 100%|██████████| 115/115 [00:00<00:00, 207.87it/s, loss=0.1131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [52/100], Loss: 0.1062, Akurasi Test (PyTorch): 95.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 53/100: 100%|██████████| 115/115 [00:00<00:00, 205.53it/s, loss=0.1481]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [53/100], Loss: 0.1416, Akurasi Test (PyTorch): 95.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 54/100: 100%|██████████| 115/115 [00:00<00:00, 212.69it/s, loss=0.1297]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [54/100], Loss: 0.1252, Akurasi Test (PyTorch): 96.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 55/100: 100%|██████████| 115/115 [00:00<00:00, 207.04it/s, loss=0.1624]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [55/100], Loss: 0.1553, Akurasi Test (PyTorch): 96.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 56/100: 100%|██████████| 115/115 [00:00<00:00, 197.71it/s, loss=0.0993]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [56/100], Loss: 0.0941, Akurasi Test (PyTorch): 95.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 57/100: 100%|██████████| 115/115 [00:00<00:00, 195.71it/s, loss=0.1043]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [57/100], Loss: 0.0926, Akurasi Test (PyTorch): 95.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 58/100: 100%|██████████| 115/115 [00:00<00:00, 192.41it/s, loss=0.1029]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [58/100], Loss: 0.0931, Akurasi Test (PyTorch): 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 59/100: 100%|██████████| 115/115 [00:00<00:00, 177.74it/s, loss=0.1126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [59/100], Loss: 0.1116, Akurasi Test (PyTorch): 95.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 60/100: 100%|██████████| 115/115 [00:00<00:00, 170.03it/s, loss=0.1627]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [60/100], Loss: 0.1542, Akurasi Test (PyTorch): 96.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 61/100: 100%|██████████| 115/115 [00:00<00:00, 202.80it/s, loss=0.1246]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [61/100], Loss: 0.1170, Akurasi Test (PyTorch): 95.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 62/100: 100%|██████████| 115/115 [00:00<00:00, 211.82it/s, loss=0.0650]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [62/100], Loss: 0.0627, Akurasi Test (PyTorch): 95.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 63/100: 100%|██████████| 115/115 [00:00<00:00, 212.27it/s, loss=0.0490]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [63/100], Loss: 0.0469, Akurasi Test (PyTorch): 95.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 64/100: 100%|██████████| 115/115 [00:00<00:00, 201.32it/s, loss=0.0680]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [64/100], Loss: 0.0626, Akurasi Test (PyTorch): 96.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 65/100: 100%|██████████| 115/115 [00:00<00:00, 207.75it/s, loss=0.0705]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [65/100], Loss: 0.0662, Akurasi Test (PyTorch): 96.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 66/100: 100%|██████████| 115/115 [00:00<00:00, 201.61it/s, loss=0.0669]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [66/100], Loss: 0.0611, Akurasi Test (PyTorch): 96.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 67/100: 100%|██████████| 115/115 [00:00<00:00, 198.93it/s, loss=0.0717]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [67/100], Loss: 0.0667, Akurasi Test (PyTorch): 96.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 68/100: 100%|██████████| 115/115 [00:00<00:00, 207.12it/s, loss=0.0574]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [68/100], Loss: 0.0549, Akurasi Test (PyTorch): 95.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 69/100: 100%|██████████| 115/115 [00:00<00:00, 206.90it/s, loss=0.0608]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [69/100], Loss: 0.0581, Akurasi Test (PyTorch): 95.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 70/100: 100%|██████████| 115/115 [00:00<00:00, 208.66it/s, loss=0.0627]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [70/100], Loss: 0.0600, Akurasi Test (PyTorch): 95.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 71/100: 100%|██████████| 115/115 [00:00<00:00, 209.14it/s, loss=0.0556]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [71/100], Loss: 0.0546, Akurasi Test (PyTorch): 95.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 72/100: 100%|██████████| 115/115 [00:00<00:00, 210.74it/s, loss=0.0594]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [72/100], Loss: 0.0578, Akurasi Test (PyTorch): 95.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 73/100: 100%|██████████| 115/115 [00:00<00:00, 206.01it/s, loss=0.0510]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [73/100], Loss: 0.0483, Akurasi Test (PyTorch): 95.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 74/100: 100%|██████████| 115/115 [00:00<00:00, 204.76it/s, loss=0.0557]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [74/100], Loss: 0.0533, Akurasi Test (PyTorch): 95.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 75/100: 100%|██████████| 115/115 [00:00<00:00, 207.71it/s, loss=0.0678]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [75/100], Loss: 0.0649, Akurasi Test (PyTorch): 95.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 76/100: 100%|██████████| 115/115 [00:00<00:00, 186.45it/s, loss=0.0792]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [76/100], Loss: 0.0709, Akurasi Test (PyTorch): 95.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 77/100: 100%|██████████| 115/115 [00:00<00:00, 177.85it/s, loss=0.0542]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [77/100], Loss: 0.0523, Akurasi Test (PyTorch): 95.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 78/100: 100%|██████████| 115/115 [00:00<00:00, 196.42it/s, loss=0.0774]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [78/100], Loss: 0.0694, Akurasi Test (PyTorch): 96.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 79/100: 100%|██████████| 115/115 [00:00<00:00, 176.21it/s, loss=0.0629]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [79/100], Loss: 0.0541, Akurasi Test (PyTorch): 95.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 80/100: 100%|██████████| 115/115 [00:00<00:00, 168.01it/s, loss=0.0729]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [80/100], Loss: 0.0672, Akurasi Test (PyTorch): 95.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 81/100: 100%|██████████| 115/115 [00:00<00:00, 204.69it/s, loss=0.0572]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [81/100], Loss: 0.0537, Akurasi Test (PyTorch): 96.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 82/100: 100%|██████████| 115/115 [00:00<00:00, 201.58it/s, loss=0.0684]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [82/100], Loss: 0.0643, Akurasi Test (PyTorch): 96.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 83/100: 100%|██████████| 115/115 [00:00<00:00, 203.57it/s, loss=0.0529]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [83/100], Loss: 0.0497, Akurasi Test (PyTorch): 95.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 84/100: 100%|██████████| 115/115 [00:00<00:00, 206.98it/s, loss=0.0572]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [84/100], Loss: 0.0552, Akurasi Test (PyTorch): 95.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 85/100: 100%|██████████| 115/115 [00:00<00:00, 198.45it/s, loss=0.0634]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [85/100], Loss: 0.0590, Akurasi Test (PyTorch): 96.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 86/100: 100%|██████████| 115/115 [00:00<00:00, 203.91it/s, loss=0.0593]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [86/100], Loss: 0.0562, Akurasi Test (PyTorch): 96.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 87/100: 100%|██████████| 115/115 [00:00<00:00, 201.12it/s, loss=0.0674]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [87/100], Loss: 0.0621, Akurasi Test (PyTorch): 96.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 88/100: 100%|██████████| 115/115 [00:00<00:00, 201.33it/s, loss=0.0733]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [88/100], Loss: 0.0688, Akurasi Test (PyTorch): 96.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 89/100: 100%|██████████| 115/115 [00:00<00:00, 205.17it/s, loss=0.0620]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [89/100], Loss: 0.0582, Akurasi Test (PyTorch): 96.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 90/100: 100%|██████████| 115/115 [00:00<00:00, 200.03it/s, loss=0.0562]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [90/100], Loss: 0.0533, Akurasi Test (PyTorch): 96.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 91/100: 100%|██████████| 115/115 [00:00<00:00, 203.62it/s, loss=0.0529]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [91/100], Loss: 0.0488, Akurasi Test (PyTorch): 96.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 92/100: 100%|██████████| 115/115 [00:00<00:00, 201.88it/s, loss=0.0534]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [92/100], Loss: 0.0483, Akurasi Test (PyTorch): 96.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 93/100: 100%|██████████| 115/115 [00:00<00:00, 197.79it/s, loss=0.0535]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [93/100], Loss: 0.0484, Akurasi Test (PyTorch): 96.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 94/100: 100%|██████████| 115/115 [00:00<00:00, 201.08it/s, loss=0.0547]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [94/100], Loss: 0.0513, Akurasi Test (PyTorch): 96.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 95/100: 100%|██████████| 115/115 [00:00<00:00, 198.39it/s, loss=0.0502]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [95/100], Loss: 0.0467, Akurasi Test (PyTorch): 95.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 96/100: 100%|██████████| 115/115 [00:00<00:00, 184.57it/s, loss=0.0421]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [96/100], Loss: 0.0384, Akurasi Test (PyTorch): 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 97/100: 100%|██████████| 115/115 [00:00<00:00, 185.03it/s, loss=0.0425]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [97/100], Loss: 0.0374, Akurasi Test (PyTorch): 95.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 98/100: 100%|██████████| 115/115 [00:00<00:00, 182.69it/s, loss=0.0526]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [98/100], Loss: 0.0453, Akurasi Test (PyTorch): 95.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 99/100: 100%|██████████| 115/115 [00:00<00:00, 171.53it/s, loss=0.0443]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [99/100], Loss: 0.0443, Akurasi Test (PyTorch): 96.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 100/100: 100%|██████████| 115/115 [00:00<00:00, 179.59it/s, loss=0.0483]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/100], Loss: 0.0466, Akurasi Test (PyTorch): 95.76%\n",
            "\n",
            "Akurasi terbaik yang dicapai: 96.27%\n",
            "\n",
            "Memuat model terbaik untuk ekspor...\n",
            "Mengekspor vektor model...\n",
            "Vektor dan parameter BatchNorm berhasil diekspor ke folder 'exported_vectors_ucihar'.\n",
            "\n",
            "Memulai verifikasi dengan inferensi manual pada seluruh data tes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Verifikasi Manual: 100%|██████████| 2947/2947 [00:00<00:00, 5220.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Hasil Verifikasi ---\n",
            "Akurasi model PyTorch (terbaik): 96.27%\n",
            "Akurasi inferensi manual (NumPy):      96.27%\n",
            "✅ Verifikasi BERHASIL: Akurasi cocok.\n",
            "\n",
            "Memulai ekspor untuk implementasi Fixed-Point...\n",
            "Data uji mentah (float) yang diekspor adalah untuk kelas: 4\n",
            "Berhasil mengekspor 'model_params_scaled.h'\n",
            "Berhasil mengekspor 'test_data.h'\n",
            "Berhasil mengekspor 'test_data.c'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q13QqKBs99ZO",
        "outputId": "973a35fa-b5d2-4d74-cbae-7cd35dfa1067"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Skrip untuk Melakukan Inferensi Langsung pada Satu Data LDC\n",
        "\n",
        "Skrip ini menyediakan class 'LDCInference' yang membungkus semua\n",
        "logika dan aset model (vektor V, F, C, dan parameter BatchNorm) untuk\n",
        "memudahkan prediksi pada data mentah tunggal.\n",
        "\n",
        "Cara kerja:\n",
        "1.  Inisialisasi class `LDCInference`. Ini akan memuat semua yang dibutuhkan.\n",
        "2.  Siapkan satu sampel data mentah (misalnya, satu baris dari file teks UCI-HAR).\n",
        "3.  Panggil metode `predict` pada sampel tersebut untuk mendapatkan hasilnya.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import os\n",
        "import torch # Diperlukan hanya untuk memuat state_dict BatchNorm\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Class Inference Engine (Final & Terverifikasi)\n",
        "# =============================================================================\n",
        "\n",
        "class LDCInference:\n",
        "    \"\"\"\n",
        "    Sebuah class untuk melakukan inferensi LDC pada data baru.\n",
        "\n",
        "    Class ini menangani pemuatan model (vektor V, F, C) dan prapemrosesan data\n",
        "    (scaling dan kuantisasi) yang konsisten dengan saat training.\n",
        "    \"\"\"\n",
        "    def __init__(self, vector_dir, training_data_for_scaler):\n",
        "        \"\"\"\n",
        "        Inisialisasi inference engine.\n",
        "\n",
        "        Args:\n",
        "            vector_dir (str): Path ke folder berisi file V, F, C .npy dan state_dict.\n",
        "            training_data_for_scaler (np.array): Data training mentah (X_train_raw)\n",
        "                yang diperlukan untuk me-fit scaler agar prapemrosesan konsisten.\n",
        "        \"\"\"\n",
        "        print(\"Menginisialisasi LDC Inference Engine...\")\n",
        "        if not os.path.exists(vector_dir):\n",
        "            raise FileNotFoundError(f\"Folder vektor '{vector_dir}' tidak ditemukan.\")\n",
        "\n",
        "        # 1. Memuat vektor V (real-valued), F (biner), C (biner)\n",
        "        self.V_real = np.load(os.path.join(vector_dir, 'V_real_value_vectors.npy'))\n",
        "        self.F = np.load(os.path.join(vector_dir, 'F_feature_vectors.npy'))\n",
        "        self.C = np.load(os.path.join(vector_dir, 'C_class_vectors.npy'))\n",
        "\n",
        "        # 2. Memuat parameter BatchNorm\n",
        "        bn_value_state = torch.load(os.path.join(vector_dir, 'bn_value_state_dict.pth'), map_location='cpu')\n",
        "        self.bn_value_mean = bn_value_state['running_mean'].numpy()\n",
        "        self.bn_value_var = bn_value_state['running_var'].numpy()\n",
        "        self.bn_value_weight = bn_value_state['weight'].numpy()\n",
        "        self.bn_value_bias = bn_value_state['bias'].numpy()\n",
        "\n",
        "        bn_feature_state = torch.load(os.path.join(vector_dir, 'bn_feature_state_dict.pth'), map_location='cpu')\n",
        "        self.bn_feature_mean = bn_feature_state['running_mean'].numpy()\n",
        "        self.bn_feature_var = bn_feature_state['running_var'].numpy()\n",
        "        self.bn_feature_weight = bn_feature_state['weight'].numpy()\n",
        "        self.bn_feature_bias = bn_feature_state['bias'].numpy()\n",
        "\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "        # 3. Menyiapkan scaler data\n",
        "        self.min_val = np.min(training_data_for_scaler, axis=0)\n",
        "        self.max_val = np.max(training_data_for_scaler, axis=0)\n",
        "        self.range_val = self.max_val - self.min_val\n",
        "        self.range_val[self.range_val == 0] = 1\n",
        "        self.num_value = 256\n",
        "\n",
        "        # 4. Nama kelas untuk interpretasi\n",
        "        self.class_names = [\n",
        "            'WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS',\n",
        "            'SITTING', 'STANDING', 'LAYING'\n",
        "        ]\n",
        "        print(\"Engine siap.\")\n",
        "\n",
        "    def predict(self, raw_sample_vector):\n",
        "        \"\"\"\n",
        "        Melakukan inferensi lengkap pada satu sampel data mentah.\n",
        "\n",
        "        Args:\n",
        "            raw_sample_vector (np.array): Vektor fitur 1D dari data mentah.\n",
        "\n",
        "        Returns:\n",
        "            str: Nama kelas yang diprediksi.\n",
        "            np.array: Skor kemiripan untuk semua kelas.\n",
        "        \"\"\"\n",
        "        # Prapemrosesan: Normalisasi dan Kuantisasi\n",
        "        normalized_sample = (raw_sample_vector - self.min_val) / self.range_val\n",
        "        quantized_sample = np.clip((normalized_sample * (self.num_value - 1)), 0, self.num_value - 1).astype(int)\n",
        "\n",
        "        # Langkah 1: ValueBox (menggunakan Vektor REAL)\n",
        "        value_vectors_combined = self.V_real[quantized_sample].flatten()\n",
        "\n",
        "        # Langkah 2: Terapkan BatchNorm untuk Value\n",
        "        bn_value_out = (value_vectors_combined - self.bn_value_mean) / np.sqrt(self.bn_value_var + self.epsilon)\n",
        "        bn_value_out = bn_value_out * self.bn_value_weight + self.bn_value_bias\n",
        "\n",
        "        # Langkah 3: FeatureLayer\n",
        "        sample_vector_S_real = bn_value_out @ self.F.T\n",
        "\n",
        "        # Langkah 4: Terapkan BatchNorm untuk Feature\n",
        "        bn_feature_out = (sample_vector_S_real - self.bn_feature_mean) / np.sqrt(self.bn_feature_var + self.epsilon)\n",
        "        bn_feature_out = bn_feature_out * self.bn_feature_weight + self.bn_feature_bias\n",
        "\n",
        "        # Langkah 5: Binarisasi\n",
        "        sample_vector_S_bin = np.sign(bn_feature_out)\n",
        "        sample_vector_S_bin[sample_vector_S_bin == 0] = 1\n",
        "\n",
        "        # Langkah 6: ClassLayer\n",
        "        similarity_scores = sample_vector_S_bin @ self.C.T\n",
        "        predicted_idx = np.argmax(similarity_scores)\n",
        "\n",
        "        return self.class_names[predicted_idx], similarity_scores\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Contoh Penggunaan\n",
        "# =============================================================================\n",
        "\n",
        "def get_data_for_setup(url='https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip', dest_path='./data'):\n",
        "    \"\"\"Fungsi helper untuk mendapatkan data yang dibutuhkan untuk inisialisasi.\"\"\"\n",
        "    extract_folder = os.path.join(dest_path, 'UCI HAR Dataset')\n",
        "    if not os.path.exists(extract_folder):\n",
        "        # Logika download sederhana jika data tidak ada\n",
        "        print(\"Data UCI-HAR tidak ditemukan. Silakan jalankan 'ldc_ucihar.py' terlebih dahulu.\")\n",
        "        return None, None, None\n",
        "\n",
        "    X_train_raw = np.loadtxt(os.path.join(extract_folder, 'train', 'X_train.txt'), dtype=np.float32)\n",
        "    X_test_raw = np.loadtxt(os.path.join(extract_folder, 'test', 'X_test.txt'), dtype=np.float32)\n",
        "    y_test_raw = np.loadtxt(os.path.join(extract_folder, 'test', 'y_test.txt'), dtype=np.int64)\n",
        "    return X_train_raw, X_test_raw, y_test_raw\n",
        "\n",
        "def main():\n",
        "    print(\"--- Contoh Penggunaan LDC Inference Engine ---\")\n",
        "\n",
        "    # --- Persiapan ---\n",
        "    vector_dir = \"exported_vectors_ucihar\"\n",
        "    X_train_raw, X_test_raw, y_test_raw = get_data_for_setup()\n",
        "\n",
        "    if X_train_raw is None: return\n",
        "\n",
        "    # --- Inisialisasi Engine ---\n",
        "    try:\n",
        "        engine = LDCInference(vector_dir, X_train_raw)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Pastikan Anda telah menjalankan 'ldc_ucihar.py' untuk menghasilkan vektor model.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Melakukan Inferensi pada Data Baru ---\")\n",
        "    # Di aplikasi nyata, 'sample_to_test' akan berasal dari sensor atau input lain.\n",
        "    # Kita ambil satu sampel acak dari data tes sebagai contoh.\n",
        "    sample_index = np.random.randint(0, len(X_test_raw))\n",
        "    sample_to_test = X_test_raw[sample_index]\n",
        "    actual_label_idx = y_test_raw[sample_index] - 1\n",
        "    actual_label_name = engine.class_names[actual_label_idx]\n",
        "\n",
        "    print(f\"Menguji sampel data #{sample_index}\")\n",
        "    print(f\"Label aktual: {actual_label_name}\")\n",
        "\n",
        "    # --- Lakukan Prediksi ---\n",
        "    predicted_class, similarity_scores = engine.predict(sample_to_test)\n",
        "\n",
        "    print(\"\\n--- Hasil Inferensi ---\")\n",
        "    print(f\"Prediksi Model: '{predicted_class}'\")\n",
        "\n",
        "    if predicted_class == actual_label_name:\n",
        "        print(\"✅ Prediksi BENAR!\")\n",
        "    else:\n",
        "        print(f\"❌ Prediksi SALAH.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab_ZIxG9YM_m",
        "outputId": "21c86fb2-c2a0-419e-c422-a4fdecaa26b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Contoh Penggunaan LDC Inference Engine ---\n",
            "Menginisialisasi LDC Inference Engine...\n",
            "Engine siap.\n",
            "\n",
            "--- Melakukan Inferensi pada Data Baru ---\n",
            "Menguji sampel data #1140\n",
            "Label aktual: WALKING\n",
            "\n",
            "--- Hasil Inferensi ---\n",
            "Prediksi Model: 'WALKING'\n",
            "✅ Prediksi BENAR!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KxOvnlwYNTp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t0zfZ_rCmyhK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEW REVISE"
      ],
      "metadata": {
        "id": "LEINIiU5SSra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Konverter Model Universal MicroVSA (Diperbaiki)\n",
        "\n",
        "Skrip ini mengambil vektor model (C, F, dan V) yang telah dilatih dan\n",
        "disimpan dalam format NumPy (.npy), lalu mengubahnya menjadi sepasang file\n",
        ".h dan .c yang siap digunakan oleh library MicroVSA di microcontroller.\n",
        "\n",
        "Versi ini memperbaiki logika preprocessor di file .c yang dihasilkan untuk\n",
        "menghindari error 'conflicting types' dan '#else without #if'.\n",
        "\n",
        "File yang Dihasilkan:\n",
        "1.  **model.h**: File header yang berisi deklarasi 'extern' untuk variabel\n",
        "    model dan makro untuk dimensi.\n",
        "2.  **model.c**: File sumber yang berisi definisi aktual (data heksadesimal)\n",
        "    dari semua matriks model.\n",
        "\n",
        "CARA MENGGUNAKAN:\n",
        "1.  Letakkan file .npy Anda (C_class_vectors.npy, dll.) di folder yang sama.\n",
        "2.  Jalankan skrip: `python universal_converter.py`\n",
        "3.  File `model.h` dan `model.c` akan dibuat/ditimpa dengan versi yang benar.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "# --- PENGATURAN ---\n",
        "VECTOR_DIR = \"/content/exported_vectors_ucihar\"\n",
        "OUTPUT_HEADER_FILENAME = \"model.h\"\n",
        "OUTPUT_SOURCE_FILENAME = \"model.c\"\n",
        "\n",
        "VAR_NAME_C = \"MICROVSA_MODEL_C\"\n",
        "VAR_NAME_F = \"MICROVSA_MODEL_F\"\n",
        "VAR_NAME_V = \"MICROVSA_MODEL_V\"\n",
        "\n",
        "C_QUALIFIER = \"MODEL_C_QUALIFIER\"\n",
        "F_QUALIFIER = \"MODEL_F_QUALIFIER\"\n",
        "V_QUALIFIER = \"MODEL_V_QUALIFIER\"\n",
        "\n",
        "# --- Bentuk Matriks Input yang Diharapkan ---\n",
        "# - C_class_vectors.npy:      (JUMLAH_KELAS, DF)\n",
        "# - F_feature_vectors.npy:      (JUMLAH_FITUR, DF)\n",
        "# - V_real_value_vectors.npy: (256, DV)\n",
        "\n",
        "def pack_bipolar_array(bipolar_arr, word_size_bits):\n",
        "    if bipolar_arr.ndim != 2:\n",
        "        raise ValueError(\"Input array harus 2D.\")\n",
        "\n",
        "    rows, cols = bipolar_arr.shape\n",
        "\n",
        "    if cols % word_size_bits != 0:\n",
        "        padding_needed = word_size_bits - (cols % word_size_bits)\n",
        "        padding_values = np.ones((rows, padding_needed), dtype=bipolar_arr.dtype)\n",
        "        padded_arr = np.hstack((bipolar_arr, padding_values))\n",
        "        print(f\"  [INFO] Menambahkan padding pada matriks shape {bipolar_arr.shape} -> {padded_arr.shape} untuk word_size {word_size_bits}\")\n",
        "        bipolar_arr = padded_arr\n",
        "        cols = bipolar_arr.shape[1]\n",
        "\n",
        "    # Konversi bipolar ke biner: -1 -> 1, 1 -> 0 (untuk ekuivalensi XOR)\n",
        "    binary_arr = np.where(bipolar_arr <= 0, 1, 0).astype(np.uint8)\n",
        "    packed_8bit = np.packbits(binary_arr, axis=-1)\n",
        "\n",
        "    if word_size_bits == 8:\n",
        "        return packed_8bit\n",
        "\n",
        "    dtype = np.uint16 if word_size_bits == 16 else np.uint32\n",
        "    bytes_per_word = word_size_bits // 8\n",
        "\n",
        "    if packed_8bit.shape[1] % bytes_per_word != 0:\n",
        "        raise ValueError(\"Packing error, shape tidak cocok setelah padding.\")\n",
        "\n",
        "    # Ubah urutan byte ke big-endian sebelum melihatnya sebagai tipe data yang lebih besar\n",
        "    packed_8bit_big_endian = packed_8bit.reshape(rows, -1, bytes_per_word)\n",
        "\n",
        "    # Buat array kosong dengan tipe data target\n",
        "    result = np.zeros((packed_8bit_big_endian.shape[0], packed_8bit_big_endian.shape[1]), dtype=dtype)\n",
        "\n",
        "    # Salin data byte-per-byte\n",
        "    for i in range(bytes_per_word):\n",
        "        result |= packed_8bit_big_endian[:, :, i].astype(dtype) << ((bytes_per_word - 1 - i) * 8)\n",
        "\n",
        "    return result\n",
        "\n",
        "def format_array_to_c_string(packed_arr, var_name, qualifier, c_type):\n",
        "    hex_formatter = {\n",
        "        'uint8_t': lambda x: f\"0x{x:02x}\",\n",
        "        'uint16_t': lambda x: f\"0x{x:04x}\",\n",
        "        'uint32_t': lambda x: f\"0x{x:08x}\",\n",
        "    }[c_type]\n",
        "    flat_list = [hex_formatter(val) for val in packed_arr.flatten()]\n",
        "    body = \", \".join(flat_list)\n",
        "    wrapped_body = \"\\n\" + textwrap.fill(body, width=80, initial_indent=\"    \", subsequent_indent=\"    \") + \"\\n\"\n",
        "    return f\"{qualifier} {c_type} {var_name}[] = {{{wrapped_body}}};\"\n",
        "\n",
        "def generate_c_code_for_matrix(f_source, matrix, var_name, qualifier, word_size):\n",
        "    c_type = f\"uint{word_size}_t\"\n",
        "    packed_normal = pack_bipolar_array(matrix, word_size)\n",
        "    c_string_normal = format_array_to_c_string(packed_normal, var_name, qualifier, c_type)\n",
        "\n",
        "    transposed_matrix = matrix.T\n",
        "    packed_transposed = pack_bipolar_array(transposed_matrix, word_size)\n",
        "    c_string_transposed = format_array_to_c_string(packed_transposed, var_name, qualifier, c_type)\n",
        "\n",
        "    var_suffix = var_name.split('_')[-1]\n",
        "    f_source.write(f\"#ifndef MODEL_TRANSPOSE_{var_suffix}\\n\")\n",
        "    f_source.write(c_string_normal)\n",
        "    f_source.write(\"\\n#else\\n\")\n",
        "    f_source.write(c_string_transposed)\n",
        "    f_source.write(\"\\n#endif\\n\\n\")\n",
        "\n",
        "def main():\n",
        "    print(\"Memulai proses konversi model MicroVSA...\")\n",
        "    try:\n",
        "        c_vec = np.load(os.path.join(VECTOR_DIR, 'C_class_vectors.npy'))\n",
        "        f_vec = np.load(os.path.join(VECTOR_DIR, 'F_feature_vectors.npy'))\n",
        "        v_vec_real = np.load(os.path.join(VECTOR_DIR, 'V_real_value_vectors.npy'))\n",
        "        print(\"Berhasil memuat file .npy.\")\n",
        "\n",
        "        v_vec = np.sign(v_vec_real); v_vec[v_vec == 0] = -1\n",
        "        num_classes, df = c_vec.shape\n",
        "        _, dv = v_vec.shape\n",
        "\n",
        "        if f_vec.shape[1] == df:\n",
        "            num_features = f_vec.shape[0]\n",
        "        elif f_vec.shape[0] == df:\n",
        "            print(\"  [INFO] Matriks F terdeteksi dalam format (DF, num_features). Mentransposisinya ke (num_features, DF).\")\n",
        "            f_vec = f_vec.T\n",
        "            num_features = f_vec.shape[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Dimensi DF dari matriks F ({f_vec.shape}) tidak cocok dengan matriks C ({c_vec.shape}).\")\n",
        "\n",
        "        print(f\"  - Dimensi terdeteksi: DF={df}, DV={dv}, Classes={num_classes}, Features={num_features}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Gagal memproses file: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- Tulis File Header (.h) ---\n",
        "    with open(OUTPUT_HEADER_FILENAME, 'w') as f_header:\n",
        "        header_guard = f\"MODEL_{os.path.basename(OUTPUT_HEADER_FILENAME).upper().replace('.', '_')}_\"\n",
        "        f_header.write(f\"#ifndef {header_guard}\\n#define {header_guard}\\n\\n\")\n",
        "        f_header.write(\"#include <stdint.h>\\n\")\n",
        "        f_header.write(\"#include \\\"microvsa_config.h\\\"\\n\\n\")\n",
        "        f_header.write(f\"#define MICROVSA_MODEL_FHV_DIMENSION_BIT {df}\\n\")\n",
        "        f_header.write(f\"#if MICROVSA_IMPL_WORDSIZE == 8\\n#define MICROVSA_MODEL_FHV_DIMENSION_WORD {df // 8}\\n\")\n",
        "        f_header.write(f\"#elif MICROVSA_IMPL_WORDSIZE == 16\\n#define MICROVSA_MODEL_FHV_DIMENSION_WORD {df // 16}\\n\")\n",
        "        f_header.write(f\"#elif MICROVSA_IMPL_WORDSIZE == 32\\n#define MICROVSA_MODEL_FHV_DIMENSION_WORD {df // 32}\\n\")\n",
        "        f_header.write(\"#else\\n# error Unsupported MICROVSA_IMPL_WORDSIZE\\n#endif\\n\\n\")\n",
        "        f_header.write(f\"#define MICROVSA_MODEL_NUM_CLASS {num_classes}\\n\")\n",
        "        f_header.write(f\"#define MICROVSA_MODEL_NUM_FEATURE {num_features}\\n\\n\")\n",
        "        f_header.write(f\"#ifdef MODEL_C_IN_RAM\\n#define {C_QUALIFIER}\\n#else\\n#define {C_QUALIFIER} const\\n#endif\\n\")\n",
        "        f_header.write(f\"#ifdef MODEL_F_IN_RAM\\n#define {F_QUALIFIER}\\n#else\\n#define {F_QUALIFIER} const\\n#endif\\n\")\n",
        "        f_header.write(f\"#ifdef MODEL_V_IN_RAM\\n#define {V_QUALIFIER}\\n#else\\n#define {V_QUALIFIER} const\\n#endif\\n\\n\")\n",
        "        f_header.write(\"#if MICROVSA_IMPL_WORDSIZE == 8\\n\")\n",
        "        f_header.write(f\"extern {C_QUALIFIER} uint8_t {VAR_NAME_C}[];\\n\")\n",
        "        f_header.write(f\"extern {F_QUALIFIER} uint8_t {VAR_NAME_F}[];\\n\")\n",
        "        f_header.write(f\"extern {V_QUALIFIER} uint8_t {VAR_NAME_V}[];\\n\")\n",
        "        f_header.write(\"#elif MICROVSA_IMPL_WORDSIZE == 16\\n\")\n",
        "        f_header.write(f\"extern {C_QUALIFIER} uint16_t {VAR_NAME_C}[];\\n\")\n",
        "        f_header.write(f\"extern {F_QUALIFIER} uint16_t {VAR_NAME_F}[];\\n\")\n",
        "        f_header.write(f\"extern {V_QUALIFIER} uint16_t {VAR_NAME_V}[];\\n\")\n",
        "        f_header.write(\"#elif MICROVSA_IMPL_WORDSIZE == 32\\n\")\n",
        "        f_header.write(f\"extern {C_QUALIFIER} uint32_t {VAR_NAME_C}[];\\n\")\n",
        "        f_header.write(f\"extern {F_QUALIFIER} uint32_t {VAR_NAME_F}[];\\n\")\n",
        "        f_header.write(f\"extern {V_QUALIFIER} uint32_t {VAR_NAME_V}[];\\n\")\n",
        "        f_header.write(\"#endif\\n\\n\")\n",
        "        f_header.write(f\"#endif // {header_guard}\\n\")\n",
        "\n",
        "    # --- Tulis File Sumber (.c) ---\n",
        "    with open(OUTPUT_SOURCE_FILENAME, 'w') as f_source:\n",
        "        f_source.write(f\"// File sumber model MicroVSA, dibuat secara otomatis.\\n\")\n",
        "        f_source.write(f\"#include \\\"{OUTPUT_HEADER_FILENAME}\\\"\\n\\n\")\n",
        "\n",
        "        # --- Blok untuk 8-bit ---\n",
        "        f_source.write(\"#if MICROVSA_IMPL_WORDSIZE == 8\\n\")\n",
        "        print(\"Memproses untuk arsitektur 8-bit...\")\n",
        "        packed_v_8 = pack_bipolar_array(v_vec, 8)\n",
        "        f_source.write(format_array_to_c_string(packed_v_8, VAR_NAME_V, V_QUALIFIER, 'uint8_t') + \"\\n\")\n",
        "        generate_c_code_for_matrix(f_source, f_vec, VAR_NAME_F, F_QUALIFIER, 8)\n",
        "        generate_c_code_for_matrix(f_source, c_vec, VAR_NAME_C, C_QUALIFIER, 8)\n",
        "\n",
        "        # --- Blok untuk 16-bit ---\n",
        "        f_source.write(\"#elif MICROVSA_IMPL_WORDSIZE == 16\\n\")\n",
        "        print(\"Memproses untuk arsitektur 16-bit...\")\n",
        "        packed_v_16 = pack_bipolar_array(v_vec, 16)\n",
        "        f_source.write(format_array_to_c_string(packed_v_16, VAR_NAME_V, V_QUALIFIER, 'uint16_t') + \"\\n\")\n",
        "        generate_c_code_for_matrix(f_source, f_vec, VAR_NAME_F, F_QUALIFIER, 16)\n",
        "        generate_c_code_for_matrix(f_source, c_vec, VAR_NAME_C, C_QUALIFIER, 16)\n",
        "\n",
        "        # --- Blok untuk 32-bit ---\n",
        "        f_source.write(\"#elif MICROVSA_IMPL_WORDSIZE == 32\\n\")\n",
        "        print(\"Memproses untuk arsitektur 32-bit...\")\n",
        "        packed_v_32 = pack_bipolar_array(v_vec, 32)\n",
        "        f_source.write(format_array_to_c_string(packed_v_32, VAR_NAME_V, V_QUALIFIER, 'uint32_t') + \"\\n\")\n",
        "        generate_c_code_for_matrix(f_source, f_vec, VAR_NAME_F, F_QUALIFIER, 32)\n",
        "        generate_c_code_for_matrix(f_source, c_vec, VAR_NAME_C, C_QUALIFIER, 32)\n",
        "\n",
        "        f_source.write(\"#endif\\n\")\n",
        "\n",
        "    print(f\"\\n✅ Berhasil! File '{OUTPUT_HEADER_FILENAME}' dan '{OUTPUT_SOURCE_FILENAME}' telah diperbarui.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXAXROQTSTl5",
        "outputId": "b51886a6-645b-4051-a30a-27e6dc31f9e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai proses konversi model MicroVSA...\n",
            "Berhasil memuat file .npy.\n",
            "  [INFO] Matriks F terdeteksi dalam format (DF, num_features). Mentransposisinya ke (num_features, DF).\n",
            "  - Dimensi terdeteksi: DF=128, DV=8, Classes=6, Features=4488\n",
            "Memproses untuk arsitektur 8-bit...\n",
            "  [INFO] Menambahkan padding pada matriks shape (128, 6) -> (128, 8) untuk word_size 8\n",
            "Memproses untuk arsitektur 16-bit...\n",
            "  [INFO] Menambahkan padding pada matriks shape (256, 8) -> (256, 16) untuk word_size 16\n",
            "  [INFO] Menambahkan padding pada matriks shape (128, 4488) -> (128, 4496) untuk word_size 16\n",
            "  [INFO] Menambahkan padding pada matriks shape (128, 6) -> (128, 16) untuk word_size 16\n",
            "Memproses untuk arsitektur 32-bit...\n",
            "  [INFO] Menambahkan padding pada matriks shape (256, 8) -> (256, 32) untuk word_size 32\n",
            "  [INFO] Menambahkan padding pada matriks shape (128, 4488) -> (128, 4512) untuk word_size 32\n",
            "  [INFO] Menambahkan padding pada matriks shape (128, 6) -> (128, 32) untuk word_size 32\n",
            "\n",
            "✅ Berhasil! File 'model.h' dan 'model.c' telah diperbarui.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 10. (FIXED-POINT + DEBUG) Ekspor Parameter & Data Uji untuk Implementasi C\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "\n",
        "print(\"Memulai ekspor untuk implementasi Fixed-Point...\")\n",
        "\n",
        "# --- Konfigurasi ---\n",
        "SCALE_FACTOR = 4096\n",
        "TEST_SAMPLE_IDX = 0\n",
        "\n",
        "# --- Perhitungan Parameter Fixed-Point ---\n",
        "try:\n",
        "    data_dir = './data/UCI HAR Dataset';\n",
        "    X_train_raw = np.loadtxt(os.path.join(data_dir, 'train', 'X_train.txt'), dtype=np.float32)\n",
        "    y_train_raw = np.loadtxt(os.path.join(data_dir, 'train', 'y_train.txt'), dtype=np.int64)\n",
        "    X_test_raw = np.loadtxt(os.path.join(data_dir, 'test', 'X_test.txt'), dtype=np.float32)\n",
        "    y_test_raw = np.loadtxt(os.path.join(data_dir, 'test', 'y_test.txt'), dtype=np.int64)\n",
        "    min_val = np.min(X_train_raw, axis=0)\n",
        "    max_val = np.max(X_train_raw, axis=0)\n",
        "    range_val = max_val - min_val\n",
        "    range_val[range_val == 0] = 1e-9\n",
        "\n",
        "    min_val_scaled = (min_val * SCALE_FACTOR).astype(np.int32)\n",
        "    inv_range_val_scaled = ((1.0 / range_val) * SCALE_FACTOR).astype(np.int32)\n",
        "    test_data_sample_raw = X_test_raw[TEST_SAMPLE_IDX]\n",
        "    # REPLACE IT WITH THIS:\n",
        "    # Use the zero-indexed y_test, which is consistent with the model's training\n",
        "    y_test = y_test_raw - 1\n",
        "    test_data_actual_label = y_test[TEST_SAMPLE_IDX]\n",
        "    print(f\"Data uji yang diekspor adalah untuk kelas: {test_data_actual_label}\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"ERROR: Pastikan 'X_train_raw' dan 'y_test' sudah ada. Error: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Proses Ekspor ke File C (Tidak ada perubahan di sini) ---\n",
        "# (Kode ekspor file .h dan .c tetap sama seperti sebelumnya)\n",
        "header_path = \"model_params_scaled.h\"\n",
        "with open(header_path, \"w\") as f:\n",
        "    f.write(\"#ifndef MODEL_PARAMS_SCALED_H_\\n#define MODEL_PARAMS_SCALED_H_\\n\\n#include <stdint.h>\\n\\n\")\n",
        "    f.write(f\"#define FIXED_POINT_SCALE_FACTOR {SCALE_FACTOR}\\n\\n\")\n",
        "    f.write(f\"const int32_t min_val_scaled[{len(min_val_scaled)}] = {{ \")\n",
        "    f.write(\", \".join(map(str, min_val_scaled)))\n",
        "    f.write(\" };\\n\\n\")\n",
        "    f.write(f\"const int32_t inv_range_val_scaled[{len(inv_range_val_scaled)}] = {{ \")\n",
        "    f.write(\", \".join(map(str, inv_range_val_scaled)))\n",
        "    f.write(\" };\\n\\n#endif // MODEL_PARAMS_SCALED_H_\\n\")\n",
        "print(f\"Berhasil mengekspor '{header_path}'\")\n",
        "header_path = \"test_data.h\"\n",
        "with open(header_path, \"w\") as f:\n",
        "    f.write(\"#ifndef TEST_DATA_H_\\n#define TEST_DATA_H_\\n\\n#include <stdint.h>\\n\\n\")\n",
        "    f.write(f\"#define TEST_DATA_SAMPLE_LENGTH {len(test_data_sample_raw)}\\n\")\n",
        "    f.write(\"extern const float test_data_sample[];\\n\")\n",
        "    f.write(f\"extern const uint8_t test_data_actual_label;\\n\\n#endif // TEST_DATA_H_\\n\")\n",
        "print(f\"Berhasil mengekspor '{header_path}'\")\n",
        "source_path = \"test_data.c\"\n",
        "with open(source_path, \"w\") as f:\n",
        "    f.write('#include \"test_data.h\"\\n\\n')\n",
        "    f.write(f\"const uint8_t test_data_actual_label = {test_data_actual_label};\\n\\n\")\n",
        "    f.write(f\"const float test_data_sample[TEST_DATA_SAMPLE_LENGTH] = {{\\n    \")\n",
        "    f.write(\", \".join([f\"{val:.8f}f\" for val in test_data_sample_raw]))\n",
        "    f.write(\"\\n};\\n\")\n",
        "print(f\"Berhasil mengekspor '{source_path}'\")\n",
        "\n",
        "# --- (BARU) VERIFIKASI GROUND TRUTH UNTUK DEBUGGING ---\n",
        "print(\"\\n--- VERIFIKASI UNTUK DEBUGGING ---\")\n",
        "print(\"Ini adalah nilai 'processed_sample' yang seharusnya dihasilkan di MCU:\")\n",
        "processed_sample_py = []\n",
        "for i in range(len(test_data_sample_raw)):\n",
        "    raw_scaled = int(test_data_sample_raw[i] * SCALE_FACTOR)\n",
        "    temp = raw_scaled - min_val_scaled[i]\n",
        "    norm_64 = int(temp) * int(inv_range_val_scaled[i]) # Gunakan perkalian integer murni\n",
        "    norm_scaled = norm_64 >> 12\n",
        "    # Tambahkan pembulatan yang benar (add half before shifting)\n",
        "    quantized_value = (norm_scaled * (256 - 1) + (SCALE_FACTOR // 2)) >> 12\n",
        "\n",
        "    if quantized_value < 0:\n",
        "        quantized_value = 0\n",
        "    elif quantized_value > 255:\n",
        "        quantized_value = 255\n",
        "    processed_sample_py.append(int(quantized_value))\n",
        "\n",
        "print(\"10 Nilai Pertama 'processed_sample' (Python):\")\n",
        "print(\", \".join(map(str, processed_sample_py[:10])))\n",
        "print(\"------------------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyxOO_uPQz5z",
        "outputId": "2ab01e07-5f5a-402b-9fc6-0c3bc816eef5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai ekspor untuk implementasi Fixed-Point...\n",
            "Data uji yang diekspor adalah untuk kelas: 4\n",
            "Berhasil mengekspor 'model_params_scaled.h'\n",
            "Berhasil mengekspor 'test_data.h'\n",
            "Berhasil mengekspor 'test_data.c'\n",
            "\n",
            "--- VERIFIKASI UNTUK DEBUGGING ---\n",
            "Ini adalah nilai 'processed_sample' yang seharusnya dihasilkan di MCU:\n",
            "10 Nilai Pertama 'processed_sample' (Python):\n",
            "160, 125, 126, 8, 11, 42, 6, 10, 42, 14\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CIS-cfMUQ0J6"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}